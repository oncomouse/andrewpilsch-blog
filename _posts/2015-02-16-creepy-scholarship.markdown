---
title: Creepy Scholarship
date: 2015-02-16 22:57 UTC
tags: scholarship, big data, algorithmic culture, information
---

There is a lot of talk of the "weird" in today's humanities. From Karen Gregory's ["Weird Solidarities"](http://dismagazine.com/issues/72958/karen-gregory-weird-solidarities/) to Graham Harman's [*Weird Realism*](http://www.amazon.com/Weird-Realism-Philosophy-Graham-Harman/dp/1780992521), we are going through a "weird" moment in humanities (we're also going through a weird moment, if you catch my drift). "Weird" is, as you probably know, entering our academic lexicon not only through our own perplexity at emerging digital culture but also specifically through an engagement with the horror fiction of H.P. Lovecraft as a possible guide to that perplexed state of affairs. 

A term I want to consider in this post, though, is another term in the horror lexicon: "creepy." "Creepy," according to the *OED*, comes from the "chill shuddering feeling caused by horror or repugnance," a "creeping of the skin." Recently, I've been thinking about creepy scholarship, as a possible related discourse to weird scholarship, sharing some of the same antecedents, though arriving at different results and deploying different methods.

As we all know, big data is often very creepy. That moment when Google successfully recommends something you were looking for without you telling it, for instance. Or when we're surreptitiously connected through Facebook with people we'd rather not be. Or any other host of uncanny activities that might produce the horror and repugnance that creeps across our skin.

With the application of big data methods to humanities scholarship, I've been finding myself running up against what we might describe as creepy scholarship. I want to provide two examples and then I conclude with some thoughts on what this means.

### Cyber-stalking Elderly Professors

Jonathan Goodwin, as I'm sure many of you saw, spent the month leading up to MLA posting screen captures from the historical JIL data that [Jim Ridolofo](http://rid.olfo.org/) produced using commercial OCR software. [Goodwin posted his findings on his blog, after MLA](http://jgoodwin.net/blog/jobs-of-the-mla/). As he mentions at the bottom of that blog post, at one point, I suggested that his project would have more exigence if we could match job listings to names.

My interest in this project was because one of the truths of the academic job market in the humanities is that "institutional fit" (one of the most nebulous terms in a [profession dominated by nebulous terms](http://www.nytimes.com/2015/02/14/opinion/digitizing-the-humanities.html)) is often so fluid. People get hired who bear little seeming resemblance to their JIL ads because departments decide or know in advance that they *actually* need someone slightly different then they put into words. There's a whole host of reasons for this slippage, and I won't get into enumerating them here. I merely point out the predominance of fit to highlight that data-mining the JIL without accounting for the reality of hiring does not give a full picture of the reality of historic job markets.

As Jonathan Goodwin notes in his piece, "the privacy issues associated with such a database are also problematic, I think." Having been heatedly chastised by [Shawna Ross](http://www.shawnaross.com) on this issue, apparently other people think this, too. For me, matching names to ads provides a further data point in unraveling the history of the professional mess we're in. Apparently, though, despite the reality of this data (you can literally go and see who any of these institutions hired), there are privacy implications I don't seem to be able to grasp with publicizing this data in an automated fashion.

So, while I may appear like a big data apologist here (how much has "information wants to be free" become a threat since the 1990s, btw?), apparently my proposing this project is an example of creepy scholarship. My next encounter with the creep, though, probably explains why.

### Bibliography as Creeping

I am a big fan, as I think I've documented on this blog, of Eugene Thacker. I argue to anyone who will listen that *In the Dust of This Planet* is the best work of critical theory of the past 5 years. His work on swarms was critical to my thinking when I started in graduate school.

Without spoiling anything, I realize I have an article on Thacker's approach to theory that fits in with the sprawling project on the inhuman I have planned. However, since leaving Georgia Tech for The New School, Thacker does not have a website. Given how much he also publishes in small press edited collections and on boutique theory sites, I realized that to say anything coherent about his project, I would need a bibliography of his work.

So, two hours later, what I thought would be about a 15 minute project was more or less finished (I say more or less because I stopped cataloguing his book reviews and interviews). Thacker has published *a lot*, and it was starting to feel a little like cyberstalking to hunt down all of his stuff. For instance, I found a few essays Google Scholar didn't index simply by asking myself "where would he publish that isn't on my list" and, sure enough, he did publish an essay in *Angelaki*.

Where this turned really creepy, though, is that in the ebb-and-flow of his bibliography, I began to discern patterns. Creepy patterns. "Wow," I thought, "2005 was a really banner year for him; must have been the big push before tenure." There's nothing wrong with the data I had compiled, just as there's nothing wrong with matching names of professors to JIL ads, but once I start to feel like this data lets me speculate about the contents of a person's inner life, there is something like a violation of privacy.

### Creepy Narratives / Distant Narratives

This idea of the creepy, emergent narrative is, of course, part of the general creepiness of big data. We're perfectly fine giving away facts about ourselves, but once we see the person-shaped impression, the blurry doppelg√§nger produced by our data, things are weird: "that person-shaped blur is not me. Or is it?"

Given this creepiness, why are we okay with big data narratives about books or about historical subjects? Is it because they're dead? If that's the case, is it better that we cannot (legally) do data-mining on post-1923 literature?

I don't have an answer to all of these questions, but I think it's an important confrontation that digital humanities needs to have with its own methods. The ease with which a simple digital project or even speculation about future projects slides into the creepy territory of distant narrative and the surveillance technologies that author them is startling.

This creep further suggests the problem of interpretation in all DH projects: if narratives extracted from the patterns of data-mining are creepy (and I think they can be), where are they creepy? when are they creepy? why might some be creepy and not others? How do we respond ethically to the age of creepy scholarship?

As a final plug, these questions could form the grist for some interesting presentations in the special session I'm proposing for MLA 16, ["Critical Informatics and the Digital Humanities."](http://oncomouse.github.io/mla16.html)